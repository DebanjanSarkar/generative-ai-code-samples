{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9f21fb-f691-43d6-930a-753f6b9b7662",
   "metadata": {},
   "source": [
    "<h1><center>LangChain 101: from Zero to Hero</center></h1>\n",
    "<hr><hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6ee89e-819e-4ccd-a127-ea8c9cb0b3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285ae18-e7b0-4e09-ba1b-2de5368588af",
   "metadata": {},
   "source": [
    "### Configurations:-\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f909183-9521-4e37-ae07-0bdaa45fbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_api_version = \"2023-05-15\"\n",
    "llm_deployment_name = os.getenv(\"GPT_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"]     = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"]  = azure_openai_api_version\n",
    "os.environ[\"OPENAI_API_KEY\"]      = azure_openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391afc30-fae9-49c3-9780-c49a74a1af20",
   "metadata": {},
   "source": [
    "## 1. The LLMs:\n",
    "----------------\n",
    "The fundamental component of LangChain involves invoking an LLM with a specific input. <br>\n",
    "To illustrate this, we'll explore a simple example. Let's imagine we are building a service that *suggests personalized workout routines based on an individual's fitness goals and preferences*.\n",
    "\n",
    "To accomplish this, we will first need to import the LLM wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534931b4-f1bd-4e99-9d59-4d8bdd4de9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b603b8-b79a-4dc3-a9d8-e82db62ed026",
   "metadata": {},
   "source": [
    "- The `temperature` parameter in OpenAI models manages the randomness of the output.\n",
    "- When `temperature` is set to 0, the output is mostly predetermined and suitable for tasks requiring stability and the most probable result. At a setting of 1.0, the output can be inconsistent and interesting but isn't generally advised for most tasks.\n",
    "- For creative tasks, a `temperature` between **0.70 and 0.90** offers a balance of reliability and creativity. The best setting should be determined by experimenting with different values for each specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f585ce-6672-43f6-91c9-01bde3a932b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_deployment = llm_deployment_name,\n",
    "    temperature = 0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fd81d-0987-4258-86fe-e756b852f3c3",
   "metadata": {},
   "source": [
    "Calling the model with some input- This code will generate a personalized workout routine based on the user's fitness goals and preferences using the LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2ae113-510f-4e14-a585-c0c35663f359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a personalized workout routine that focuses on improving cardiovascular endurance while incorporating outdoor activities:\n",
      "\n",
      "1. Warm-up (5-10 minutes):\n",
      "   - Start with a light jog or brisk walk to warm up your muscles.\n",
      "   - Perform dynamic stretches like leg swings, arm circles, and lunges.\n",
      "\n",
      "2. Outdoor Running (3 times a week):\n",
      "   - Choose a scenic route or park with varied terrain.\n",
      "   - Start with a comfortable pace and gradually increase your speed over time.\n",
      "   - Incorporate interval training by alternating between a moderate pace and a faster, more challenging pace for specific time intervals (e.g., 1 minute fast, 2 minutes moderate).\n",
      "   - Aim for a minimum of 30 minutes of continuous running per session, gradually increasing the duration as you progress.\n",
      "\n",
      "3. Cycling (2 times a week):\n",
      "   - Invest in a bicycle or rent one from a local rental service.\n",
      "   - Explore different cycling routes, such as bike trails or scenic roads.\n",
      "   - Start with moderate cycling speed and gradually increase your intensity.\n",
      "   - Aim for 45-60 minutes of continuous cycling per session, incorporating uphill climbs and sprints to challenge yourself.\n",
      "\n",
      "4. Hiking (1-2 times a week):\n",
      "   - Find nearby hiking trails or nature parks to explore.\n",
      "   - Choose trails with varying terrains and elevation to challenge your endurance.\n",
      "   - Focus on maintaining a steady pace throughout the hike, and try to increase your speed over time.\n",
      "   - Aim for hikes between 60-90 minutes, gradually increasing the duration and difficulty as you progress.\n",
      "\n",
      "5. Interval Training (2 times a week):\n",
      "   - Choose a spacious outdoor area, such as a park or open field.\n",
      "   - Perform exercises like sprinting, high knees, jumping jacks, burpees, and mountain climbers, with short rest intervals in between.\n",
      "   - Start with 10-15 seconds of high-intensity activity, followed by 30 seconds of rest, and repeat the circuit for 10-15 minutes.\n",
      "   - As you progress, increase the duration and intensity of each exercise.\n",
      "\n",
      "6. Cool-down and Stretching (5-10 minutes):\n",
      "   - After each workout, finish with a slow-paced jog or walk to cool down.\n",
      "   - Perform static stretches to target major muscle groups, holding each stretch for 20-30 seconds.\n",
      "\n",
      "Remember to listen to your body, stay hydrated, and gradually increase the intensity and duration of your workouts to avoid injuries. It's also recommended to consult with a fitness professional before starting any new workout routine.\n"
     ]
    }
   ],
   "source": [
    "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
    "\n",
    "response = llm.invoke( text )\n",
    "print( response.content )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ab1fc-833f-4651-b900-c95c94ad4290",
   "metadata": {},
   "source": [
    "## 2. The Chains:\n",
    "------------------\n",
    "In LangChain, a chain is an end-to-end wrapper around multiple individual components, providing a way to accomplish a common use case by combining these components in a specific sequence. The most commonly used type of chain is the LLMChain, which consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser.\n",
    "\n",
    "The LLMChain works as follows:\n",
    "1. Takes (multiple) input variables.\n",
    "2. Uses the PromptTemplate to format the input variables into a prompt.\n",
    "3. Passes the formatted prompt to the model (LLM or ChatModel).\n",
    "4. If an output parser is provided, it uses the OutputParser to parse the output of the LLM into a final format.\n",
    "\n",
    "Below is the code to create a chain that generates a possible name for a company that produces eco-friendly water bottles. By using LangChain's LLMChain, PromptTemplate, and OpenAIclasses, we can easily define our prompt, set the input variables, and generate creative outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50cad2a6-b393-4f04-9f70-4695b59cd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc18386-8f3a-4eb3-a606-f2c9c6d8c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_deployment = llm_deployment_name,\n",
    "    temperature = 0.9\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "\n",
    "llm_chain = LLMChain( prompt=prompt, llm=llm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "408f7bc9-91a5-4b53-88bc-3250bcf7a5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product': 'eco-friendly water bottles', 'text': 'EcoAqua'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "response = llm_chain.invoke(\"eco-friendly water bottles\")\n",
    "print( response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e36de7a-199c-448c-a428-31dfd18a145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EcoAqua\n"
     ]
    }
   ],
   "source": [
    "print( response[\"text\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debc3631-4fc2-4407-bb34-e1c1cd814eeb",
   "metadata": {},
   "source": [
    "The same chain can also be built as follow:-\n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "899e9f5e-ce76-46d6-b069-58202522e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d5b35b-5689-40ab-99c0-19aba938512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "What is a good name for a company that makes {product}?\n",
    "\"\"\")\n",
    "\n",
    "company_name_chain = company_name_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc2f76f-f08e-4f04-9293-2e53057f69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Organic Preservative-free ayurvedic supplements\"\n",
    "\n",
    "# Invoking the chain to get response from llm\n",
    "response = company_name_chain.invoke( {\"product\": product} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fab2b337-a67b-4e87-a533-74aa3892d395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Nature's Ayurblends\"\n"
     ]
    }
   ],
   "source": [
    "print( response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27155872-d10d-42b1-b6f7-7b8f29d8d83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nature's Ayurblends\n"
     ]
    }
   ],
   "source": [
    "print( response.content )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042a867-0944-4894-8f26-19ee47c41983",
   "metadata": {},
   "source": [
    "## 3. The Memory:-\n",
    "-----------------------\n",
    "- In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI.\n",
    "- It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses.\n",
    "- Memory, such as `ConversationBufferMemory`, acts as a wrapper around `ChatMessageHistory`, extracting the messages and providing them to the chain for better context-aware generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502e3b9a-426c-4a5e-b92c-1e84b803e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d8ba4b1-cc82-4796-a6ac-5acb1485d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_deployment = llm_deployment_name,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c300cbe-f117-47f5-90f1-636ec97ce909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Tell me about yourself.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about yourself.\n",
      "AI: I'm an AI language model developed by OpenAI called GPT-3. I have been trained on a wide range of data sources, including books, articles, and websites, to generate human-like responses. I can understand and generate text in multiple languages, and I can assist with a variety of tasks, such as answering questions, providing explanations, and even engaging in creative writing. Is there anything specific you would like to know or discuss?\n",
      "Human: What can you do?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Tell me about yourself.\n",
      "AI: I'm an AI language model developed by OpenAI called GPT-3. I have been trained on a wide range of data sources, including books, articles, and websites, to generate human-like responses. I can understand and generate text in multiple languages, and I can assist with a variety of tasks, such as answering questions, providing explanations, and even engaging in creative writing. Is there anything specific you would like to know or discuss?\n",
      "Human: What can you do?\n",
      "AI: I can do a lot of things! As an AI language model, I can assist with various tasks. For example, I can help answer questions, provide explanations, generate text, and even engage in creative writing. If you have any specific requests or topics you'd like to discuss, feel free to let me know!\n",
      "Human: How can you help me with data analysis?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I can assist with data analysis in a few different ways. For example, if you have a dataset and you're not sure how to approach analyzing it, I can help you brainstorm different approaches and techniques. I can also help you with specific tasks, such as cleaning and preprocessing data, performing statistical analysis, or creating visualizations to better understand your data. Additionally, if you have any questions about data analysis concepts or methods, I can provide explanations and examples to help clarify things for you. Just let me know what specific help you need, and I'll do my best to assist you!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the conversation\n",
    "conversation.predict(input=\"Tell me about yourself.\")\n",
    "\n",
    "# Continue the conversation\n",
    "conversation.predict(input=\"What can you do?\")\n",
    "conversation.predict(input=\"How can you help me with data analysis?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39916d7c-7bcc-4c76-bad8-cd1af8ba7758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself.'), AIMessage(content=\"I'm an AI language model developed by OpenAI called GPT-3. I have been trained on a wide range of data sources, including books, articles, and websites, to generate human-like responses. I can understand and generate text in multiple languages, and I can assist with a variety of tasks, such as answering questions, providing explanations, and even engaging in creative writing. Is there anything specific you would like to know or discuss?\"), HumanMessage(content='What can you do?'), AIMessage(content=\"I can do a lot of things! As an AI language model, I can assist with various tasks. For example, I can help answer questions, provide explanations, generate text, and even engage in creative writing. If you have any specific requests or topics you'd like to discuss, feel free to let me know!\"), HumanMessage(content='How can you help me with data analysis?'), AIMessage(content=\"I can assist with data analysis in a few different ways. For example, if you have a dataset and you're not sure how to approach analyzing it, I can help you brainstorm different approaches and techniques. I can also help you with specific tasks, such as cleaning and preprocessing data, performing statistical analysis, or creating visualizations to better understand your data. Additionally, if you have any questions about data analysis concepts or methods, I can provide explanations and examples to help clarify things for you. Just let me know what specific help you need, and I'll do my best to assist you!\")])) verbose=True llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000236DCFDFFA0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000236DCFE1420>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', azure_endpoint='https://gpt-demo-openai.openai.azure.com/', deployment_name='gptturbo', openai_api_version='2023-05-15', openai_api_type='azure')\n"
     ]
    }
   ],
   "source": [
    "# Display the conversation\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a93dec-9981-4fd2-9239-cc1729359f02",
   "metadata": {},
   "source": [
    "Points to note:-\n",
    "---------------------\n",
    "1. In the above output, you can see the memory being used by observing the \"Current conversation\" section.\n",
    "2. After each input from the user, the conversation is updated with both the user's input and the AI's response.\n",
    "3. This way, the memory maintains a record of the entire conversation.\n",
    "4. When the AI generates its next response, it will use this conversation history as context, making its responses more coherent and relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b00d79-eba1-4cba-afa4-f53633b2cc8f",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14220734-2888-432f-9f92-2219039d98a8",
   "metadata": {},
   "source": [
    "# Deep Lake VectorStore:\n",
    "---------------------------\n",
    "Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps. It enables hybrid searches on these embeddings and their attributes for efficient data retrieval. It also integrates with LangChain, facilitating the development and deployment of applications.\n",
    "\n",
    "Deep Lake provides several advantages over the typical vector store:\n",
    "\n",
    "- **It’s multimodal**, which means that it can be used to store items of diverse modalities, such as texts, images, audio, and video, along with their vector representations. \n",
    "- **It’s serverless**, which means that we can create and manage cloud datasets without creating and managing a database instance. This aspect gives a great speedup to new projects.\n",
    "- Last, it’s possible to **easily create a data loader** out of the data loaded into a Deep Lake dataset. It is convenient for fine-tuning machine learning models using common frameworks like *PyTorch* and *TensorFlow*.\n",
    "\n",
    "Install deeplake package:\n",
    "- `pip install deeplake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d0c79a-5ddd-4d00-a1d1-ec97597fec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: deeplake\n",
      "Version: 3.8.22\n",
      "Summary: Activeloop Deep Lake\n",
      "Home-page: \n",
      "Author: activeloop.ai\n",
      "Author-email: support@activeloop.ai\n",
      "License: MPL-2.0\n",
      "Location: e:\\programs & codes\\generative ai\\_genai_venv\\lib\\site-packages\n",
      "Requires: boto3, click, humbug, lz4, numpy, pathos, pillow, pydantic, pyjwt, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65a67c5f-d906-4eff-b785-907882506999",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_deployment_name = os.getenv(\"GPT_DEPLOYMENT_NAME\")\n",
    "embedding_model_deployment_name = os.getenv(\"AZURE_OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "activeloop_token = os.getenv(\"ACTIVELOOP_DEEPLAKE_KEY\")\n",
    "\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = activeloop_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "280ae11b-bafd-4fc7-8c86-cfec16751e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f230bb-d9c5-41c2-ae60-389975fe61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_deployment = llm_deployment_name,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embedding_model_deployment_name,\n",
    "    openai_api_version=azure_openai_api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d579ed5-e7ef-44e3-ab93-0b5db7b448ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19561bca-1816-4f7a-b43a-7ad417c4ff09",
   "metadata": {},
   "source": [
    "### Create deeplake dataset:\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14fec9c2-d79b-4434-8a84-a73d32a0b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "# my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\" \n",
    "my_activeloop_org_id = \"debanjanbusy\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eba5bc13-a3e6-4070-ac6c-191032b38eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.deeplake.DeepLake at 0x1ff46839480>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafefad-0743-479e-9e03-e9c789cf37b2",
   "metadata": {},
   "source": [
    "### Delete a deeplake dataset:\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d5af31-0614-4dc2-b6d3-5d0ecbd3aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8583b8ab-1026-4311-b8d3-477f306f6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_activeloop_org_id = \"debanjanbusy\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eb3f8b3-dbb1-4175-8a93-bad81856b333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "deeplake.delete( dataset_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26ad41-989c-4366-837c-0e6a78f182a0",
   "metadata": {},
   "source": [
    "### Add documents to our deeplake dataset:\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bc6b961-96d3-4afc-a680-ce023ee1bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9399f23f-ed04-4a68-874b-062b4d50ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 2 embeddings in 1 batches of size 2:: 100%|█████████████████████████████████████| 1/1 [00:21<00:00, 21.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://debanjanbusy/langchain_course_from_zero_to_hero', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (2, 1)      str     None   \n",
      " metadata     json      (2, 1)      str     None   \n",
      " embedding  embedding  (2, 1536)  float32   None   \n",
      "    id        text      (2, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# add documents to our Deep Lake dataset\n",
    "response = db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90bbef-4804-4e44-9ba2-7f1a5f3307ae",
   "metadata": {},
   "source": [
    "- The output obtained executing the code in the above cell looks somewhat like this: <br>\n",
    "<img src=\"./images/01-output of adding documents to dataset.jpg\" width=\"1000px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c0a7858-7e74-45c6-a3be-5e9f6920f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7bdcfde6-d6d5-11ee-8dd0-80913334ccee', '7bdcfde7-d6d5-11ee-8351-80913334ccee']\n"
     ]
    }
   ],
   "source": [
    "print( response )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311bfc7-dcb1-4b5a-a75f-edddc435f64c",
   "metadata": {},
   "source": [
    "## Visualizing the Dataset using `dataset.visualize()`:\n",
    "--------------------------------------------------------\n",
    "- `visualize()` method requires Python `flask` module to visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa8f60b-4d92-4708-b9a6-f236511e5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Flask\n",
      "Version: 3.0.2\n",
      "Summary: A simple framework for building complex web applications.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: e:\\programs & codes\\generative ai\\_genai_venv\\lib\\site-packages\n",
      "Requires: blinker, click, itsdangerous, Jinja2, Werkzeug\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd7ad6c-d86c-4de6-9ef7-151649ff1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61ec4d59-d83f-4e51-a032-8abff0fd23d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/debanjanbusy/langchain_course_from_zero_to_hero\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://debanjanbusy/langchain_course_from_zero_to_hero loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "my_dataset = deeplake.load(\"hub://debanjanbusy/langchain_course_from_zero_to_hero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d523706f-ea00-46ab-8af1-e4877d6ca964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HINT: Please forward the port - 50455 to your local machine, if you are running on the cloud.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"90%\"\n",
       "            height=\"800\"\n",
       "            src=\"https://app.activeloop.ai/visualizer/hub?url=hub://debanjanbusy/langchain_course_from_zero_to_hero&token=eyJhbGciOiJub25lIiwidHlwIjoiSldUIn0.eyJpZCI6ImRlYmFuamFuYnVzeSIsImFwaV9rZXkiOiJjNVBuLVdIbGVQWU0ybEJMUHBXSlJHS0g0RWRmaFhNRkhPWDkzWlFqYjlkVFkifQ.\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x21110bf8070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'dataset_visualizer'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "my_dataset.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731b1e6-4bdc-4cf2-9cbc-a31e73041e69",
   "metadata": {},
   "source": [
    "## Creating a `RetrievalQA` chain:\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f81352-5e51-4187-b765-5562d8ad8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "296d4ba6-7c90-4adb-b2b7-bc9a5717f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_deployment = llm_deployment_name,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embedding_model_deployment_name,\n",
    "    openai_api_version=azure_openai_api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8666304-89eb-44a8-85db-973dfddbb7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://debanjanbusy/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "my_activeloop_org_id = \"debanjanbusy\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# Loading an existing non-empty dataset.\n",
    "db = DeepLake(dataset_path=dataset_path, read_only=True, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd861d7-e0f1-4c06-9c46-5fca425eef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.deeplake.DeepLake object at 0x0000014A735DBE20>\n"
     ]
    }
   ],
   "source": [
    "print( db )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1072882f-a687-4f06-8f92-11605ba4c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "\tllm=llm,\n",
    "\tchain_type=\"stuff\",\n",
    "\tretriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604ae7f-2a65-4df8-8dc5-2cda52a511b3",
   "metadata": {},
   "source": [
    "#### Creating an agent that uses the above `RetrievalQA` chain as a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3958640e-5cad-4a10-b888-4d753de1a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab734fb-7f70-434f-b6bd-945f733266da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.invoke,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "\ttools,\n",
    "\tllm,\n",
    "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "\tverbose=True\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d18d86-1e7d-40ec-bbed-9cb8a25e1d33",
   "metadata": {},
   "source": [
    "- Using the agent to ask a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29d0772d-b1fe-469c-9fea-227a05385f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the Retrieval QA System to find the answer to this question.\n",
      "Action: Retrieval QA System\n",
      "Action Input: \"When was Napoleone born?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'query': 'When was Napoleone born?', 'result': 'Napoleon Bonaparte was born on 15 August 1769.'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe answer to the question \"When was Napoleone born?\" is \"Napoleon Bonaparte was born on 15 August 1769.\"\n",
      "Final Answer: Napoleon Bonaparte was born on 15 August 1769.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'When was Napoleone born?', 'output': 'Napoleon Bonaparte was born on 15 August 1769.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\"When was Napoleone born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e16653f-b4ba-447b-83cc-30d8546000b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Napoleon Bonaparte was born on 15 August 1769.\n"
     ]
    }
   ],
   "source": [
    "print( response[\"output\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4901e84-e553-4f92-95c6-4c29459f4a48",
   "metadata": {},
   "source": [
    "- The example above how to use Deep Lake as a vector database and create an agent with a RetrievalQA chain as a tool to answer questions based on the given document.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe50570-69fc-4a5c-87fd-5d29bcc43e4c",
   "metadata": {},
   "source": [
    "## Reloading an existing vector store/dataset and adding more data to it:\n",
    "--------------------------------------------------------------------------\n",
    "- We first reload an existing vector store from Deep Lake that's located at a specified dataset path.\n",
    "- Then, we load new textual data and split it into manageable chunks.\n",
    "- Finally, we add these chunks to the existing dataset, creating and storing corresponding embeddings for each added text segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daf51a5d-cdff-47a1-965d-3a21ce10747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://debanjanbusy/langchain_course_from_zero_to_hero already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "my_activeloop_org_id = \"debanjanbusy\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# load the existing Deep Lake dataset and specify the embedding function\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2016e763-542b-45cf-933c-53bb0b4e18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new documents\n",
    "texts = [\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c47ee02b-6c8a-459e-8bb4-151afcefb135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 2 embeddings in 1 batches of size 2:: 100%|█████████████████████████████████████| 1/1 [00:29<00:00, 29.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://debanjanbusy/langchain_course_from_zero_to_hero', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c0df26a0-d712-11ee-a9be-80913334ccef',\n",
       " 'c0df26a1-d712-11ee-bbc9-80913334ccef']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f0520-5347-4ec0-87a2-da7b1d9c569e",
   "metadata": {},
   "source": [
    "- Shape = (4,1536) for embedding means there are 4 embeddings (one per document), and each embedding has 1536 columns, i.e., 1536 elementary data(float) in each row.\n",
    "- Shape = (4,1) for text means there are 4 texts / documents, and each document has 1 column, i.e., single elementary data and not an array or collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a081e-c994-4c0b-b4c8-57f7febcdb4e",
   "metadata": {},
   "source": [
    "#### We then recreate our previous agent and ask a question that can be answered only by the last documents added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8a44ad7-3ba8-4583-8db8-37b31152b4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.invoke,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "\ttools,\n",
    "\tllm,\n",
    "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "\tverbose=True\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80295301-bd48-4ecd-8ebc-e6b97c565288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the Retrieval QA System to find the answer to this question.\n",
      "Action: Retrieval QA System\n",
      "Action Input: \"When was Michael Jeffrey Jordan born?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'query': 'When was Michael Jeffrey Jordan born?', 'result': 'Michael Jeffrey Jordan was born on February 17, 1963.'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Michael Jeffrey Jordan was born on February 17, 1963.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'When was Michael Jeffrey Jordan born?', 'output': 'Michael Jeffrey Jordan was born on February 17, 1963.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\"When was Michael Jeffrey Jordan born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489795ad-aee0-4ee3-95cb-febe7f08076e",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f28ea2-a6d7-4eac-b6d9-8d36a244897d",
   "metadata": {},
   "source": [
    "## Agents in LangChain:\n",
    "------------------------\n",
    "In LangChain, agents are high-level components that use language models (LLMs) to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning it to the user. Tools are functions that perform specific duties, such as Google Search, database lookups, or Python REPL.\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
    "\n",
    "Several types of agents are available in LangChain:\n",
    "\n",
    "- The `zero-shot-react-description` agent uses the ReAct framework to decide which tool to employ based purely on the tool's description. It necessitates a description of each tool.\n",
    "- The `react-docstore` agent engages with a docstore through the ReAct framework. It needs two tools: a Search tool and a Lookup tool. The Search tool finds a document, and the Lookup tool searches for a term in the most recently discovered document.\n",
    "- The `self-ask-with-search` agent employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original self-ask with the search paper, where a Google search API was provided as the tool.\n",
    "- The `conversational-react-description` agent is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions.\n",
    "\n",
    "In the below example, the Agent will use the Google Search tool to look up recent information about the Mars rover and generates a response based on this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0f7fd-7edb-4819-ae68-9332aa2500c8",
   "metadata": {},
   "source": [
    "### Building Agent using `PythonREPL` Tool:-\n",
    "----------------------------------------------\n",
    "- The `PythonREPL` tool is contained within the `langchain_experimental` module, so it needs to be installed first.\n",
    "- Sometimes, for complex calculations, rather than have an LLM generate the answer directly, it can be better to have the LLM generate code to calculate the answer, and then run that code to get the answer. In order to easily do that, we provide a simple Python REPL to execute commands in.\n",
    "\n",
    "- This interface will only return things that are printed - therefore, **if you want to use it to calculate an answer, make sure to have it print out the answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c72a410c-ef4b-438c-986c-260953c46ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-experimental\n",
      "Version: 0.0.53\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: e:\\programs & codes\\generative ai\\_genai_venv\\lib\\site-packages\n",
      "Requires: langchain, langchain-core\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84231886-59ef-43c7-8538-7660466c5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecfd3dca-be95-4e70-ad9f-5e5cd95ea93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl = PythonREPL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f97b7bd-3bb0-4680-bca5-40591d67d77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = python_repl.run(\"print(1+1)\")\n",
    "print( response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a322fb2f-23c1-4971-84a0-89a6a1514d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_code = \"\"\"\n",
    "sum = 0\n",
    "for i in range(1,11):\n",
    "    sum += i\n",
    "print(sum)\n",
    "\"\"\"\n",
    "\n",
    "response = python_repl.run( sum_code )\n",
    "print( response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94d1c67f-1078-49c7-9ae4-6f7aa3627f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del repl_tools\n",
    "# del python_repl_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a37da94f-ecbc-4e26-aeda-7b94d2998806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create the tool to pass to an agent\n",
    "repl_tools = [\n",
    "    Tool(\n",
    "        name=\"python_repl\",\n",
    "        description=\"A Python interpreter. Use this to execute python code. Input should be a valid python code. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
    "        func=python_repl.run,\n",
    "    ),\n",
    "]\n",
    "\n",
    "python_repl_agent = initialize_agent(\n",
    "\trepl_tools,\n",
    "\tllm,\n",
    "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "\tverbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "394b94e2-8f12-4052-822e-fe214d431032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a number whose factorial is to be calculated:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe code calculates the factorial of 5. I can run the code to see the result.\n",
      "\n",
      "Action: python_repl\n",
      "Action Input:\n",
      "fact = 1\n",
      "for i in range(1,6):\n",
      "    fact *= i\n",
      "print( fact )\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m120\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe code correctly calculates the factorial of 5, which is 120.\n",
      "Final Answer: 120\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '\\nfact = 1\\nfor i in range(1,6):\\n    fact *= i\\nprint( fact )\\n', 'output': '120'}\n"
     ]
    }
   ],
   "source": [
    "n = int(input(\"Enter a number whose factorial is to be calculated: \"))\n",
    "\n",
    "factorial_code = f\"\"\"\n",
    "fact = 1\n",
    "for i in range(1,{n+1}):\n",
    "    fact *= i\n",
    "print( fact )\n",
    "\"\"\"\n",
    "\n",
    "response = python_repl_agent.invoke( factorial_code )\n",
    "print( response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "640a1ab2-fac2-4790-8879-9f53e2346d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print( response[\"output\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2551bc-88b6-4cfb-b5ca-2eac0e93b894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
