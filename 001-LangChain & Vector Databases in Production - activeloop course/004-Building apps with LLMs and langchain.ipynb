{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42dcd117-434d-452f-aff0-e831966d0926",
   "metadata": {},
   "source": [
    "<h1><center>Building Applications Powered by LLMs with LangChain</center></h1>\n",
    "<hr><hr><hr>\n",
    "<h3>Introduction</h3>\n",
    "- LangChain is designed to assist developers in building end-to-end applications using language models. It offers an array of tools, components, and interfaces that simplify the process of creating applications powered by large language models and chat models. <br>\n",
    "- LangChain streamlines managing interactions with LLMs, chaining together multiple components, and integrating additional resources, such as APIs and databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9725cb-bb3e-4efb-abc9-e6db452cd70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e53c3c-47e3-4129-a088-17f19fa4a91c",
   "metadata": {},
   "source": [
    "### Configurations:-\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39c3a75-00d2-4dd6-9235-9d1b65090985",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_api_version = \"2023-05-15\"\n",
    "llm_deployment_name = os.getenv(\"GPT_DEPLOYMENT_NAME\")\n",
    "embedding_model_deployment_name = os.getenv(\"AZURE_OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"]     = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"]  = azure_openai_api_version\n",
    "os.environ[\"OPENAI_API_KEY\"]      = azure_openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6129c63-2580-4686-b263-c241db36cb0e",
   "metadata": {},
   "source": [
    "## Prompt use case:\n",
    "----------------------\n",
    "- A key feature of LangChain is its support for prompts, which encompasses:\n",
    "    1. Prompt management,\n",
    "    2. Prompt optimization, and\n",
    "    3. A generic interface for all LLMs.\n",
    "\n",
    "    The framework also provides common utilities for working with LLMs.\n",
    "\n",
    "- `ChatPromptTemplate` is used to create a structured conversation with the AI model, making it easier to manage the flow and content of the conversation. In LangChain, message prompt templates are used to construct and work with prompts, allowing us to exploit the underlying chat model's potential fully.\n",
    "\n",
    "- System and Human prompts differ in their roles and purposes when interacting with chat models. `SystemMessagePromptTemplate` provides initial instructions, context, or data for the AI model, while `HumanMessagePromptTemplate` are messages from the user that the AI model responds to.\n",
    "\n",
    "- Using the `to_messages` object in LangChain allows you to convert the formatted value of a chat prompt template into a list of message objects. This is useful when working with chat models, as it provides a structured way to manage the conversation and ensures that the chat model can understand the context and roles of the messages.\n",
    "\n",
    "#### To illustrate it, letâ€™s create a chat-based assistant that helps users find information about movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac85aae-2e2a-48c3-8902-9eb33df30f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d573849f-e40d-42a8-9069-cd655184bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_deployment = llm_deployment_name,\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7947cfb4-f125-48d6-8171-71911377707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are an assistant that helps users find information about movies.'), HumanMessage(content='Find information about the movie Inception.')]\n"
     ]
    }
   ],
   "source": [
    "system_template = \"You are an assistant that helps users find information about movies.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template = \"Find information about the movie {movie_title}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chat_message = chat_prompt.format_prompt(movie_title=\"Inception\").to_messages()\n",
    "print( chat_message )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b76491a-3561-4b19-bf6d-6537046c00c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Inception\" is a science fiction action film directed by Christopher Nolan. It was released in 2010 and stars Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page, Tom Hardy, and Marion Cotillard. The film follows a professional thief who steals information by infiltrating the subconscious of his targets through their dreams. \n",
      "\n",
      "The story revolves around Dom Cobb (played by DiCaprio), who is offered a chance to have his criminal history erased in exchange for performing the act of \"inception\" - planting an idea in someone's mind rather than stealing it. As Cobb and his team delve deeper into the layers of dreams, they face various challenges and encounter unexpected twists.\n",
      "\n",
      "\"Inception\" received critical acclaim for its originality, visual effects, and complex narrative. It won four Academy Awards, including Best Cinematography, Best Sound Editing, Best Sound Mixing, and Best Visual Effects. The film was also a commercial success, grossing over $828 million worldwide.\n",
      "\n",
      "If you are interested in watching \"Inception,\" it is available on various streaming platforms and can be rented or purchased on digital platforms.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke( chat_message )\n",
    "print( response.content )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dbc947-45d4-4dc0-8fcd-2b32a2a782ba",
   "metadata": {},
   "source": [
    "## Summarization chain example:-\n",
    "----------------------------------\n",
    "- LangChain prompts can be found in various use cases, such as summarization or question-answering chains.\n",
    "- For example, when creating a summarization chain, LangChain enables interaction with an external data source to fetch data for use in the generation step. This could involve summarizing a lengthy piece of text or answering questions using specific data sources.\n",
    "\n",
    "\n",
    "- The following code will initialize the language model using `AzureChatOpenAI` class with a temperature of 0 - because we want deterministic output.\n",
    "- The `load_summarize_chain` function accepts an instance of the language model and returns a pre-built summarization chain.\n",
    "- Lastly, the `PyPDFLoader` class is responsible for loading PDF files and converting them into a format suitable for processing by LangChain. To use it, install `PyPDF` using `pip install pypdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ea2054-d7c6-403c-8b90-ae2c5a588640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pypdf\n",
      "Version: 4.1.0\n",
      "Summary: A pure-python PDF library capable of splitting, merging, cropping, and transforming PDF files\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Mathieu Fenniak <biziqe@mathieu.fenniak.net>\n",
      "License: \n",
      "Location: e:\\programs & codes\\generative ai\\_genai_venv\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7220f552-8b3a-426d-8a56-f9e42c8cf92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_deployment = llm_deployment_name,\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d81932b9-2ab4-4fab-afd7-a70ec002a457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter path of PDF file to be summarized:  E:\\Programs & Codes\\Generative AI\\pdf_docs\\AI - PSO.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:-\n",
      "Particle Swarm Optimization (PSO) is a population-based metaheuristic strategy for solving continuous nonlinear optimization problems. It is inspired by the collective social behaviors of swarm movements, such as birds or fish searching for food. In PSO, particles represent potential solutions, and a swarm is formed by a set of particles initially distributed randomly in a search space. The position of each particle is updated based on the experiences of its neighbors, including itself. Each particle has knowledge of its own fitness value, its locally best solution, and the globally best solution of the swarm. PSO algorithm involves the sharing of information about global fitness. The concepts of memory influence and inertia weight are also discussed in relation to PSO.\n"
     ]
    }
   ],
   "source": [
    "# Load the summarization chain\n",
    "summarize_chain = load_summarize_chain(llm)\n",
    "\n",
    "# Ask user to enter the PDF file path that is to be summarised.\n",
    "pdf_file_path = input(\"Enter path of PDF file to be summarized: \")\n",
    "\n",
    "# Load the document using PyPDFLoader\n",
    "document_loader = PyPDFLoader(file_path=pdf_file_path)\n",
    "complete_document = document_loader.load()\n",
    "\n",
    "\n",
    "# Summarize the document\n",
    "summary = summarize_chain.invoke( complete_document )\n",
    "print(\"Summary:-\",summary['output_text'], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36c171-dad9-4399-87a1-9cd34ff03973",
   "metadata": {},
   "source": [
    "## QA chain example:\n",
    "------------------------\n",
    "- We can also use LangChain to manage prompts for asking general questions from the LLMs.\n",
    "- These models are proficient in addressing fundamental inquiries.\n",
    "- Nevertheless, it is crucial to remain mindful of the potential issue of hallucinations, where the models may generate non-factual information. To address this concern, we will later introduce the Retrieval chain as a means to overcome this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee0f923-c1a6-4144-bcc7-cb4690229fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_deployment = llm_deployment_name,\n",
    "    temperature = 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cbb56a-1a24-4383-a124-c66b7954bf6d",
   "metadata": {},
   "source": [
    "- We define a custom prompt template by creating an instance of the `PromptTemplate` class.\n",
    "- The template string contains a placeholder `{question}` for the input question, followed by a newline character and the `\"Answer:\"` label.\n",
    "- The `input_variables` argument is set to the list of available placeholders in the prompt (like a question in this case) to indicate the name of the variable that the chain will replace in the template `.invoke()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8caea238-006c-4b7f-a78e-6b73faf5bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=\"Question: {question}\\nAnswer:\", input_variables=[\"question\"])\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "418e7fdc-baf8-43e0-bc27-d020b5ccd507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Identify the coordinates of the two points given. Let's call them (x1, y1) and (x2, y2).\n",
      "\n",
      "2. Calculate the slope of the line using the formula: slope = (y2 - y1) / (x2 - x1). This formula represents the change in y divided by the change in x between the two points.\n",
      "\n",
      "3. Determine the y-intercept of the line using the formula: y-intercept = y1 - slope * x1. This formula represents the value of y when x is equal to 0.\n",
      "\n",
      "4. Use the slope and y-intercept to write the equation of the line in slope-intercept form: y = mx + b, where m is the slope and b is the y-intercept.\n",
      "\n",
      "5. Plot the two points on the XY plane.\n",
      "\n",
      "6. Use the equation of the line to find additional points on the line by substituting different x-values into the equation and solving for y.\n",
      "\n",
      "7. Connect the points on the XY plane to form a straight line passing through the two given points.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke( \"Explain in a pointwise manner, how can we plot a straight line on a XY plane, passing through 2 points with only coordinates of that 2 points given?\" )\n",
    "print( response[\"text\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470dcf4-293d-4b9b-b6f2-ed2ed1238d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
