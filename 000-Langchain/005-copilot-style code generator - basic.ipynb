{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56738bcb-5cd2-4970-9fcc-274fe65b0278",
   "metadata": {},
   "source": [
    "<center><h1>Code Generator Tool</h1></center>\n",
    "<hr><hr><hr>\n",
    "<ul>\n",
    "    <li>This notebook is made to use langchain library and open ai models, to build a AI driven code generation tool, that takes natural language prompt from user that for what thing he/she needs to write a code, in any computer Programming language.</li>\n",
    "    <li>The output is a pure code in a specific programming-language, without any description or messages.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8a5223-b633-4c00-af02-908d3a024a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbf2e7a-f67f-4d52-981d-0c0965742800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "api_version = \"2023-05-15\"\n",
    "\n",
    "# For working with AzureOpenAI, in place of model, the deployment name is used\n",
    "deployment_name = os.getenv(\"DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7007f800-3aaa-4817-8f56-73c9f977c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_TYPE\"]     = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"]  = api_version\n",
    "os.environ[\"OPENAI_API_KEY\"]      = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afff2f5a-2c68-40f0-bee6-9fce782ad055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,\n",
    "    azure_deployment=deployment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3eca0e-aa5d-4ec4-977e-9010426dcec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a code in Python 3 that can print all the prime numbers between 1 to 100.\"\n",
    "\n",
    "print( llm.invoke( question ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "841a7a30-caa4-4da2-aba2-75edab77c235",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def is_prime(n):\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    for i in range(2, int(n**0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "for num in range(1, 101):\n",
      "    if is_prime(num):\n",
      "        print(num)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"def is_prime(n):\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nfor num in range(1, 101):\\n    if is_prime(num):\\n        print(num)\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ac0d1-6b6a-4a3b-b18f-6c5765e58249",
   "metadata": {},
   "source": [
    "Some parts that can be added to the prompt:\n",
    "- Evaluate the output of the based on the description, and compare the desired output with the output of the program created, and if the outputs do not match, try recreating the code again. Once they match, give the return the final version of the code in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d36b50a-b848-4616-846b-b2a860c10066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "code_gen_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Based on the below description given to you, write a code that can perform exactly as the code description mentions. The code created from the description should be optimised and Time complexity efficient.\n",
    "\n",
    "<description>\n",
    "{input}\n",
    "</description>\n",
    "\n",
    "The code created should strictly follow the instructions mentioned below:\n",
    "**Instructions**\n",
    "1. Your output should contain only the code, and not any type of introduction or description at the beginning or at the end.\n",
    "2. If any Programming language is mentioned in the description, create the code specific to that language strictly, else create the code for same description that runs on Python 3.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d53539b2-4446-4142-9320-dbb3931966fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_chain = code_gen_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8df061ba-7c82-4ca3-a241-3bd101071569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter description of the code that you want:\n",
      " def bubble_sort(array):\n"
     ]
    }
   ],
   "source": [
    "code_description = input(\"Enter description of the code that you want:\\n\")\n",
    "\n",
    "response = code_chain.invoke( {\"input\": code_description} )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee834f-b1c0-4912-81fa-cddf56ef8209",
   "metadata": {},
   "source": [
    "Sample inputs:\n",
    "- Write a program in Python, to print 35 terms of the fibonacci series.\n",
    "- Write a Python program to print the multiplication table of a number till 20.\n",
    "- Write a recursive Python program, to calculate the factorial of a number.\n",
    "- Write a program to calculate HCF and LCM of 3 numbers.\n",
    "- def merge_sort(array):\n",
    "- def bubble_sort(array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea79635-43e3-4803-9afc-dd977d9d1819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def bubble_sort(array):\n",
      "    n = len(array)\n",
      "    for i in range(n):\n",
      "        # Flag to check if any swap is made in the current iteration\n",
      "        swap_made = False\n",
      "        for j in range(0, n-i-1):\n",
      "            if array[j] > array[j+1]:\n",
      "                # Swap elements\n",
      "                array[j], array[j+1] = array[j+1], array[j]\n",
      "                # Set swap_made flag to True\n",
      "                swap_made = True\n",
      "        # If no swap is made in the current iteration, array is already sorted\n",
      "        if not swap_made:\n",
      "            break\n",
      "    return array\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print( response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd376a40-1910-4e3e-bf1f-5729a182a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_sort(array):\n",
    "    n = len(array)\n",
    "    for i in range(n):\n",
    "        # Flag to check if any swap is made in the current iteration\n",
    "        swap_made = False\n",
    "        for j in range(0, n-i-1):\n",
    "            if array[j] > array[j+1]:\n",
    "                # Swap elements\n",
    "                array[j], array[j+1] = array[j+1], array[j]\n",
    "                # Set swap_made flag to True\n",
    "                swap_made = True\n",
    "        # If no swap is made in the current iteration, array is already sorted\n",
    "        if not swap_made:\n",
    "            break\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec540ab-46e3-4230-bb44-396b84c4c0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-22, -4, 0, 1, 12, 15, 19, 19, 23, 24, 25, 31, 44, 67, 76, 89]\n"
     ]
    }
   ],
   "source": [
    "ar = [12,15,25,19,-22, 76,44,31,23,89,1, 0, -4, 19, 67, 24]\n",
    "ar2 = bubble_sort(ar)\n",
    "print(ar2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35667deb-61de-4bce-acc6-61888b726ede",
   "metadata": {},
   "source": [
    "## Making the chain aware of the conversation history, including all the past messages (till a specific conversation length) in a `chat_history` list:\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- In this, user would be able to put up a follow up question, as the output will be in the memory of the llm.\n",
    "- We will add a manual logic, such that after a specific length of conversation, the older chat messages will be removed from the chat history, such that the prompt size remains in the range of 4000 tokens, as GPT-3.5 can only process context size of 4000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14f9f974-c5a4-4119-b7c4-c91c8ea747a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "code_gen_prompt_with_chat_history = ChatPromptTemplate.from_template(\"\"\"\n",
    "<|im_start|>system\n",
    "You are an expert Computer Programmer, and you are proficient in all kinds of programming languages. Your task is to create a perfect code based on description provided to you.\n",
    "\n",
    "The code created should strictly follow the instructions mentioned below:\n",
    "**Instructions**\n",
    "1. Your output should contain only the code, and not any type of introduction or description at the beginning or at the end.\n",
    "2. If any Programming language is mentioned in the description, create the code specific to that language strictly, else create the code that runs on Python 3.\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "<chat_history>\n",
    "{chat_history}\n",
    "</chat_history>\n",
    "\n",
    "Based on the below description given to you, write a code that can perform exactly as the code description mentions. The code created from the description should be optimised and Time complexity efficient.\n",
    "<description>\n",
    "{input}\n",
    "</description>\n",
    "<|im_end|>\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bea84bc-0987-41f0-96c4-3fcdb5f34157",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_gen_chain_history_aware = code_gen_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56d42c35-7b6f-4c4f-a40d-63249e54f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c74408-0d22-4cc9-935f-5d8e826b9366",
   "metadata": {},
   "source": [
    "- With each user input and reply by the assistant, we will keep appending the User Message and the AI Assisntant message in the `chat_history` list in sequence.\n",
    "- Here, to restrict the prompt size crossing the token limit, we will limit the `chat_history` to contain latest 4 set of conversations (last 4 user inputs and last 4 ai message). Thus, whenever we have 8 elements in the `chat_history` list (4 user inputs + 4 ai message) we will remove the oldest set of message pair (1 input and 1 reply), from the `chat_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d921d3b4-9357-4f58-bd12-a34d26881746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Code Description:\n",
      " quit\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "while True:\n",
    "    if len(chat_history) > 8:\n",
    "        # Removing first user-input and ai message after 4 pairs of conversation.\n",
    "        chat_history = chat_history[2::]\n",
    "    \n",
    "    code_description = input(\"Code Description:\\n\")\n",
    "    if code_description.lower() in (\"quit\", \"close\", \"exit\"):\n",
    "        break\n",
    "    \n",
    "    response = code_gen_chain_history_aware.invoke( {\"input\": code_description, \"chat_history\": chat_history} )\n",
    "    print(\"\\nAI Code Generator:\")\n",
    "    print(response)\n",
    "\n",
    "    chat_history.append( HumanMessage( content=code_description ) )\n",
    "    chat_history.append( AIMessage( content=response ) )\n",
    "\n",
    "    print(\"=====================================================================================================\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ae59e3d-d7b9-4bb7-b3c5-46b05c0f48a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Add driver code to call the above function for numbers from 1 to 5'), AIMessage(content='for i in range(1, 6):\\n    print(i)'), HumanMessage(content='Driver program should call \"factorial\" named function for all numbers between 1 to 5'), AIMessage(content='```python\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\nfor i in range(1, 6):\\n    print(factorial(i))\\n```'), HumanMessage(content='modify the driver program to change the numbers from 1 to 5 to 10 to 15'), AIMessage(content='for i in range(1, 6):\\n    print(i * 5)')]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb0d1a5-1c89-4d04-90c5-2b64b553762d",
   "metadata": {},
   "source": [
    "## Chat aware code generation assistant, that accounts entire chat history using vector selection of relevant data from entire chat history:\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "- Here, no part of chat history messages will be removed.\n",
    "- When the chat history becomes bigger, the entire chat history data will be fed to FAISS to get vector store of the entire `chat_history`, and from that entire chat history, a relevant context will be drawn out, which will be fed to `context` field of the prompt, such that token limit of model does not exceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3dce6b5-b368-4198-a154-e49882bfb9ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "code_gen_prompt_with_context = ChatPromptTemplate.from_template(\"\"\"\n",
    "<|im_start|>system\n",
    "You are an expert Computer Programmer, and you are proficient in all kinds of programming languages. Your task is to create a perfect code based on description provided to you.\n",
    "\n",
    "The code created should strictly follow the instructions mentioned below:\n",
    "**Instructions**\n",
    "1. Your output should contain only the code, and not any type of introduction or description at the beginning or at the end.\n",
    "2. If any Programming language is mentioned in the description, create the code specific to that language strictly, else create the code that runs on Python 3.\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Below is the context that contains relevant parts of the conversation history. Use the context as an additional knowledge data for code generation, if the user input references any case from the context below.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Based on the below description given to you, write a code that can perform exactly as the code description mentions. The code created from the description should be optimised and Time complexity efficient.\n",
    "<description>\n",
    "{input}\n",
    "</description>\n",
    "<|im_end|>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33475b55-75b3-430b-b225-dd3ca3923221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embedding-model instance\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da528134-12f2-43f9-a045-d812b24b5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vector index and vectorstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# code_splitter = RecursiveCharacterTextSplitter()\n",
    "# python_codes = code_splitter.from_language(language=Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdbc3d-edbe-4a9f-8f95-52c19f771e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = FAISS.from_texts(python_codes, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae75ac-c4c7-43bb-a50a-eec0eee88b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This \"retriever\" will take user \"input\" and return the related data according to the question, which will be passed as \"context\" to the \"document_chain\"\n",
    "# retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecedc06-324b-4933-a918-4107a649cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "# Obtained retriever in above case will be used here\n",
    "# retriever = vector.as_retriever()\n",
    "\n",
    "# history_aware_retriever = create_history_aware_retriever(llm, retriever, retriever_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82f2bb-9fcd-409f-a9ab-8ce6b9fe25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "code_gen_prompt_with_context = ChatPromptTemplate.from_template(\"\"\"\n",
    "<|im_start|>system\n",
    "You are an expert Computer Programmer, and you are proficient in all kinds of programming languages. Your task is to create a perfect code based on description provided to you.\n",
    "\n",
    "The code created should strictly follow the instructions mentioned below:\n",
    "**Instructions**\n",
    "1. Your output should contain only the code, and not any type of introduction or description at the beginning or at the end.\n",
    "2. If any Programming language is mentioned in the description, create the code specific to that language strictly, else create the code that runs on Python 3.\n",
    "<|im_end|>\n",
    "\n",
    "<|im_start|>user\n",
    "Below is the context that contains relevant parts of the conversation history. Use the context as an additional knowledge data for code generation, if the user input references any case from the context below.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Based on the below description given to you, write a code that can perform exactly as the code description mentions. The code created from the description should be optimised and Time complexity efficient.\n",
    "<description>\n",
    "{input}\n",
    "</description>\n",
    "<|im_end|>\n",
    "\"\"\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "code_gen_context_aware = code_gen_prompt_with_context | llm | output_parser\n",
    "\n",
    "# history_aware_retrieval_chain = create_retrieval_chain(history_aware_retriever, code_gen_context_aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9be39-ad71-4e26-b234-2fe346f6708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = \"\"\n",
    "context = \"\"\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON)\n",
    "\n",
    "while True:\n",
    "    if len(chat_history) > 0:\n",
    "        # If chat_history has some contents, i.e., 2nd user input onwards, this block will vectorize the chat_history\n",
    "        python_codes = code_splitter.split_text( chat_history )\n",
    "        vector = FAISS.from_texts(python_codes, embeddings_model)\n",
    "        retriever = vector.as_retriever()\n",
    "        history_aware_retriever = create_history_aware_retriever(llm, retriever, retriever_prompt)\n",
    "        history_aware_retrieval_chain = create_retrieval_chain(history_aware_retriever, code_gen_context_aware)\n",
    "        \n",
    "    \n",
    "    code_description = input(\"Code Description:\\n\")\n",
    "    if code_description.lower() in (\"quit\", \"close\", \"exit\"):\n",
    "        break\n",
    "    \n",
    "    response = code_gen_chain_history_aware.invoke( {\"input\": code_description, \"chat_history\": chat_history} )\n",
    "    print(\"\\nAI Code Generator:\")\n",
    "    print(response)\n",
    "\n",
    "    # chat_history = chat_history + code_description + \" | \"\n",
    "    chat_history = chat_history + \"\\n# Start of code\" AIMessage( content=response ) + \"\\n# End of code\"\n",
    "\n",
    "    print(\"=====================================================================================================\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
