{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4dcd00-92da-4512-8660-48915efb365b",
   "metadata": {},
   "source": [
    "<h1><center>LangChain Expression Language - LCEL</center></h1>\n",
    "<hr><hr>\n",
    "LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cce532-4afe-435e-acaf-47fec15feb30",
   "metadata": {},
   "source": [
    "#### Runnable:\n",
    "-----------------\n",
    "A unit of work that can be invoked, batched, streamed, transformed and composed.<br>\n",
    "Ex- A `Prompt`, `LLMChain` or any other type of chain, `ChatModel`(like `ChatOpenAI`, `AzureChatOpenAI`) are all examples of runnable, and they can be used/operated by the following methods.\n",
    "\n",
    "- `invoke`/`ainvoke`: Transforms a single input into an output.\n",
    "- `batch`/`abatch`: Efficiently transforms multiple inputs into outputs.\n",
    "- `stream`/`astream`: Streams output from a single input as it’s produced.\n",
    "- `astream_log`: Streams output and selected intermediate results from an input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597036d5-ed6d-428e-865e-cf6e941a9f83",
   "metadata": {},
   "source": [
    "### Configurations:-\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6e6931-4c91-491b-8244-a06f57da92c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48823d1d-e7c1-4202-a356-9b176aff49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_api_version = \"2023-05-15\"\n",
    "llm_deployment_name = os.getenv(\"GPT_DEPLOYMENT_NAME\")\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"]     = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"]  = azure_openai_api_version\n",
    "os.environ[\"OPENAI_API_KEY\"]      = azure_openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef834fcd-25d6-42ee-a9d9-56c7a969cc45",
   "metadata": {},
   "source": [
    "## A. Basic example: prompt + model + output parser\n",
    "-----------------------------------------------------\n",
    "The most basic and common use case is chaining a prompt template and a model together. To see how this works, let’s create a chain that takes a topic and generates a joke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2c221cc9-8572-4bc2-8b9c-56eeaf508097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_deployment=llm_deployment_name,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5c70994a-b84b-443d-bc09-c20816f45498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"\"\"Tell me a joke about {joke_topic}\"\"\")\n",
    "op_parser = StrOutputParser()\n",
    "\n",
    "joke_chain = joke_prompt | chat_model | op_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372c9b6-6204-427f-ae0f-0c5a5dc432d8",
   "metadata": {},
   "source": [
    "### i. Using `.invoke()` method:\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bcdbc0eb-17f0-44b9-91d0-d339d8e6b7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "joke_topic = \"Software Development\"\n",
    "\n",
    "response = joke_chain.invoke( {\"joke_topic\": joke_topic} )\n",
    "print( response )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e73d7-1bbe-49e4-b2b9-66f7284cd313",
   "metadata": {},
   "source": [
    "### ii. Using `.batch()` method:\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d0f6a2e1-e241-45d6-8fee-0ec4b741bc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\", 'Why did the English football team bring string to the game?\\n\\nBecause they wanted to tie the score!', 'Why did the cricket go to school?\\n\\nBecause it wanted to learn how to catch a fly ball!']\n"
     ]
    }
   ],
   "source": [
    "response = joke_chain.batch( [{\"joke_topic\": \"Cat\"}, {\"joke_topic\": \"England\"}, {\"joke_topic\": \"Cricket\"}] )\n",
    "print( response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf413454-56fb-4751-8568-91117c74c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't cats play poker in the wild?\n",
      "\n",
      "Too many cheetahs! \n",
      "\n",
      "\n",
      "\n",
      "Why did the English football team bring string to the game?\n",
      "\n",
      "Because they wanted to tie the score! \n",
      "\n",
      "\n",
      "\n",
      "Why did the cricket go to school?\n",
      "\n",
      "Because it wanted to learn how to catch a fly ball! \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in response:\n",
    "    print(_, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54996721-74f6-4455-bbe4-9657333d80ce",
   "metadata": {},
   "source": [
    "### iii. Using `.stream()` method:\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf655764-19b5-452b-8586-e3550a82dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object RunnableSequence.stream at 0x000001C92D606730>\n"
     ]
    }
   ],
   "source": [
    "stream_response = joke_chain.stream( {\"joke_topic\": \"Women\"} )\n",
    "print( stream_response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "12c39991-2d6a-47d4-8cd5-7958ebe7dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why_ did_ the_ girl_ bring_ a_ ladder_ to_ the_ bar_?\n",
      "\n",
      "_Because_ she_ heard_ the_ drinks_ were_ on_ the_ house_!__"
     ]
    }
   ],
   "source": [
    "for chunk in stream_response:\n",
    "    print(chunk, end=\"_\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a566fb-7030-4ddc-9ca8-d5ef3b2bb192",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "The `|` symbol is similar to a **unix pipe operator**, which chains together the different components **feeds the output from one component as input into the next component**.\n",
    "\n",
    "In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let’s take a look at each component individually to really understand what’s going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5691f86-7dac-4199-abde-70a671a208ad",
   "metadata": {},
   "source": [
    "### 1. Prompt:\n",
    "----------------\n",
    "- `prompt` is a `BasePromptTemplate`, which means it takes in a dictionary of template variables and produces a `PromptValue`.\n",
    "- A `PromptValue` is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or `ChatModel` (which takes a sequence of messages as input).\n",
    "- It can work with either language model type because it defines logic both for producing `BaseMessages` and for producing a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052ea87e-8c88-4fdc-b4e5-6e97ae6b0469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me a joke about ice cream')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = joke_prompt.invoke({\"joke_topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea7121d-a4a4-4a4a-8e23-8a64cd7cebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Tell me a joke about ice cream')]\n"
     ]
    }
   ],
   "source": [
    "print( prompt_value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6a98e6-0037-4c03-8651-67a6afc58da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me a joke about ice cream')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47726231-e453-430a-9b99-98cfb4e6000f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me a joke about ice cream'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4169577-eefd-4d78-ba3c-70fa0f48aecf",
   "metadata": {},
   "source": [
    "### 2. Model:\n",
    "---------------\n",
    "- The `PromptValue` is then passed to model. In this case our model is a `ChatModel`, meaning it will output a `BaseMessage` (subcategory being `AIMessage`).\n",
    "- If our model was an `LLM`, it would output a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69667338-8f80-4dc2-bd06-b2daeb7140f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream go to therapy?\\n\\nBecause it had too many rocky road trips!')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = chat_model.invoke( prompt_value )\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a20c34b0-af43-4eda-ad73-fbea083398fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why did the ice cream go to therapy?\\n\\nBecause it had too many rocky road trips!'\n"
     ]
    }
   ],
   "source": [
    "print( message )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cdb982-7033-4d0d-b675-1e907f11634f",
   "metadata": {},
   "source": [
    "### 3. Output parser:\n",
    "----------------------\n",
    "And lastly we pass our model output to the `op_parser`, which is a `BaseOutputParser` meaning it takes either a `string` or a `BaseMessage` as input. The `StrOutputParser` specifically simple converts any input into a `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd238561-72b7-4e97-9c94-9ea4d482709c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy?\\n\\nBecause it had too many rocky road trips!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_parser.invoke( message )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b195fa7-41cc-46a7-a574-a2d85b415c70",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820433c-821a-48c0-8995-efcb36ca12e8",
   "metadata": {},
   "source": [
    "## B. RAG Search Example:\n",
    "--------------------------\n",
    "For our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "381a64cb-ed09-40e8-9203-975f8537465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: faiss-cpu\n",
      "Version: 1.8.0\n",
      "Summary: A library for efficient similarity search and clustering of dense vectors.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Kota Yamaguchi <yamaguchi_kota@cyberagent.co.jp>\n",
      "License: MIT License\n",
      "Location: e:\\programs & codes\\generative ai\\_genai_venv\\lib\\site-packages\n",
      "Requires: numpy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e7fc592-78e0-494d-a3b3-eecc3bd0db0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: docarray\n",
      "Version: 0.40.0\n",
      "Summary: The data structure for multimodal data\n",
      "Home-page: https://docs.docarray.org/\n",
      "Author: DocArray\n",
      "Author-email: \n",
      "License: Apache 2.0\n",
      "Location: e:\\programs & codes\\generative ai\\_genai_venv\\lib\\site-packages\n",
      "Requires: numpy, orjson, pydantic, rich, types-requests, typing-inspect\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eef8220d-621a-4277-92e8-8b5fe8d3cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-text-splitters\n",
      "Version: 0.0.1\n",
      "Summary: LangChain text splitting utilities\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: e:\\programs & codes\\generative ai\\_genai_venv\\lib\\site-packages\n",
      "Requires: langchain-core\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "735c9a48-4e04-470a-b072-68c0df8f62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "\n",
    "embedding_deployment_name = os.getenv(\"AZURE_OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f85e4cc-e4f2-4ec9-9088-6f6215a4cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = AzureChatOpenAI(\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_deployment=llm_deployment_name,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=embedding_deployment_name,\n",
    "    openai_api_version=azure_openai_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "097eb39f-6d5c-4fe2-a941-f3974b4988f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho\\nbears like to eat honey\\n', metadata={'source': './data/001-context_data.txt'})]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text_document = TextLoader('./data/001-context_data.txt').load()\n",
    "\n",
    "raw_text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7756cab0-1595-4bbe-8b21-bb151cb2d655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print( type(raw_text_document[0].page_content) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "52a8419f-267a-4a88-9ca2-16d5808a7fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho\\nbears like to eat honey', metadata={'source': './data/001-context_data.txt'})]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\"\\n\")\n",
    "# text_splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "texts = text_splitter.split_documents(raw_text_document)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bbdf53b7-5215-4310-b2c7-8189f5526766",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents( texts, embeddings )\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b7feaf40-4d7d-40b5-bb20-f779bc2228d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "chat_chain = setup_and_retrieval | chat_prompt | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be176963-b308-49bc-833d-b91673ef7d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat_chain.invoke(\"where did harrison work?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58ed2a-fda7-4a8f-ad23-5223badd221e",
   "metadata": {},
   "source": [
    "To explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context.\n",
    "\n",
    "As a preliminary step, we’ve setup the `retriever` using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8470e9f9-eb18-4f23-bed3-de0799e3bf3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='harrison worked at kensho\\nbears like to eat honey', metadata={'source': './data/001-context_data.txt'})]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retriever.invoke(\"where did harrison work?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee911acc-ef8f-43fe-8f34-921b1dea8434",
   "metadata": {},
   "source": [
    "We then use the `RunnableParallel` to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and `RunnablePassthrough` to pass the user’s question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42e028-5937-430d-9af9-5a2ce03db05c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
