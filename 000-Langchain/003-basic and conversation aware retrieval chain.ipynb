{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1258dc13-659b-4e17-8dab-b9b4e1ab007f",
   "metadata": {},
   "source": [
    "<center><h1>Langchain Retrieval Chain Guide:</h1></center>\n",
    "<hr><hr><hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9589c7e1-1525-4f31-abed-06bab3732ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa708cde-b44d-4471-a7ec-8c1d401b877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "api_version = \"2023-05-15\"\n",
    "\n",
    "# For working with AzureOpenAI, in place of model, the deployment name is used\n",
    "deployment_name = os.getenv(\"DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a115c085-6015-4e26-ad8f-2cbfe72df393",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_TYPE\"]     = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"]  = api_version\n",
    "os.environ[\"OPENAI_API_KEY\"]      = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8493812b-f1d3-4efa-9356-c936acebe926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Integral calculus is a branch of calculus that focuses on the concept of integration. Integration deals with finding the antiderivative of a function, which is the reverse process of differentiation. It involves determining the area under a curve or the accumulation of quantities over a given interval. Integration is used to solve problems related to areas, volumes, work, and other applications in mathematics, physics, engineering, and economics. It is an essential tool in mathematical analysis and is often used in conjunction with differential calculus to solve complex problems.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,\n",
    "    azure_deployment=deployment_name,\n",
    ")\n",
    "\n",
    "question = \"What is Integral calculus?\"\n",
    "\n",
    "print( llm.invoke( question ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bdaf5-d557-4fdd-a492-d9f07bb4065b",
   "metadata": {},
   "source": [
    "# Guiding LLM response using prompt template:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80bccd-7aa4-47e6-99f0-5effdf815e9f",
   "metadata": {},
   "source": [
    "### Creating langchain prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275baddf-841b-443c-9e9e-7cbab8a16ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The {input} placeholder will be replaced by user's input using chain.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "my_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "124a52d3-0aea-48a9-b868-491f8513aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain = my_prompt | llm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35815c9-1346-4f03-ba18-e45c2eeac7c5",
   "metadata": {},
   "source": [
    "- Using the chain `my_chain` created above, we can pass the question to the `input` placeholder in the prompt `my_prompt`\n",
    "- As system messge is also passed, we will get a more refined wel-formatted answer in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9db8f75c-b7dd-46f6-a6fc-9bf6ff7615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = my_chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0adf6ec-b008-46c1-aa1b-973d29d6d3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can greatly assist with testing in multiple ways:\\n\\n1. Test Case Documentation: Langsmith can generate comprehensive test case documentation by analyzing the software codebase. It can automatically identify the different modules, classes, and functions within the code, and generate test cases for each of them. This documentation provides a systematic approach to testing and ensures that all areas of the software are thoroughly tested.\\n\\n2. Test Data Generation: Langsmith can also assist in generating test data for various scenarios. By analyzing the code, it can identify the different types of input parameters and their possible values. Based on this analysis, Langsmith can generate a wide range of test data to cover different test scenarios, including boundary cases, edge cases, and negative test cases.\\n\\n3. Test Automation: Langsmith can help in automating the testing process by generating code snippets for test scripts. It can analyze the codebase to identify the relevant sections that need to be tested and generate the necessary code for setting up test environments, executing test cases, and verifying the expected results. This automation saves time and effort in manual testing and ensures consistency in the testing process.\\n\\n4. Test Coverage Analysis: Langsmith can analyze the codebase and provide insights into the test coverage. It can identify which parts of the code are covered by the existing test cases and which areas need additional testing. This analysis helps in prioritizing testing efforts and ensures that all critical areas of the software are thoroughly tested.\\n\\n5. Integration Testing: Langsmith can assist in integration testing by analyzing the dependencies between different modules or components of the software. It can identify the required interfaces and generate test cases to validate the integration between these components. This ensures that the software functions correctly as a whole and all interactions between different modules are properly tested.\\n\\nOverall, Langsmith's capabilities in generating test case documentation, test data, automation code, test coverage analysis, and integration testing assistance make it a valuable tool for enhancing the testing process and ensuring the software's quality and reliability.\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97fe0e36-0e81-4cbe-8c58-d84688e7824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print( type(response) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b58ce-6e33-42de-b698-dfdf51338a50",
   "metadata": {},
   "source": [
    "- The output of a ChatModel ( `my_chain.invoke()` function call ) is a `AIMessage` object, as can be seen above.\n",
    "- To get this output as a string, we can use output parsers.\n",
    "- We can use the output parser combined with the chain `my_chain` such that it directly gives the output as a String, and not any type of object.\n",
    "\n",
    "<img src=\"./images/02. langchain_model-io.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6c322c5-9dfc-4733-ad61-74a6ff459e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "207f5b25-c2dc-4d82-8e91-93928544d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_chain2 = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bad00cc5-a2f7-4c76-8653-63f1fbacfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = my_chain2.invoke( {\"input\": \"How docker solves the problem related to environment setup?\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0209016-85e2-4b63-a7be-91ef1c843591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Docker solves the problem of environment setup by providing a lightweight and isolated containerization platform. Traditionally, setting up a development or production environment can be a complex and time-consuming process, involving the installation and configuration of various dependencies, libraries, and tools.\\n\\nWith Docker, developers can define the entire environment in a Dockerfile, which is a text file that specifies the base image, dependencies, and configurations required for the application to run. Docker uses a layered file system and containerization technology to package the application and its dependencies into a portable container.\\n\\nThis container can then be easily shared and deployed across different environments, such as development, testing, and production. Docker ensures that the environment is consistent and reproducible, as the container includes all the required dependencies and configurations, eliminating the need for manual setup.\\n\\nFurthermore, Docker allows developers to create and manage multiple containers on a single host, providing isolation between applications. This means that each application runs in its own container with its own set of dependencies, avoiding conflicts and ensuring that changes made to one application do not affect others.\\n\\nDocker also provides a robust ecosystem with a vast collection of pre-built images available on Docker Hub. These images can be used as a base to build customized containers, saving time and effort in setting up the environment from scratch.\\n\\nOverall, Docker simplifies the environment setup process by providing a consistent, portable, and isolated containerization platform, allowing developers to focus on building and deploying their applications rather than dealing with complex environment configurations.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "225f88a3-1145-49d2-a613-389226c61988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print( type(response2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6214cc-ae4f-4c9d-bb3b-23427318d0cc",
   "metadata": {},
   "source": [
    "# Basic Retrieval Chain :\n",
    "- **Retrieval** is useful when you have **too much data** to pass to the LLM directly, such as, when we need to pass context on which the result should be based upon.\n",
    "- We can use a retriever to fetch only the most relevant pieces of data from the entire context and pass those to the llm. This reduces number of tokens used.\n",
    "- **Context** - For example, we have a document (word or pdf or some other form) containing details of a thing, and we want to query based on the details in the document. Thus, our answer should be from the details in that document. *That document data will be referred to as the context.*\n",
    "- The **Retriever context** can be a local document, a document or webpage on the internet, an SQL table, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b135be7-0343-4e5a-ab91-a07c564ae62f",
   "metadata": {},
   "source": [
    "### Basic Retrieval Chain made with `create_retrieval_chain()` function must have following placeholder's in the prompt used in them:\n",
    "- `{context}`\n",
    "- `{input}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdffac-ec19-4ff3-a98f-4ddb22cd9bba",
   "metadata": {},
   "source": [
    "Requirements:-\n",
    "- `pip install beautifulsoup4`\n",
    "- `pip install faiss-cpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d45c32-94f3-4bb3-9f4c-960dd156a610",
   "metadata": {},
   "source": [
    "## Setting a web document as Retriever context, and use it to answer queries:\n",
    "- In this process, we will look up relevant documents from a Retriever and then pass them into the prompt.\n",
    "- A Retriever can be backed by anything - a SQL table, the internet, etc.\n",
    "- In this example, we will populate a vector store and use that as a retriever, thus, here, **the retriever context will be a vector store.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8efa20-c41d-4019-937e-1266179db04b",
   "metadata": {},
   "source": [
    "### loading the webpage data into a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "235a6213-16ef-4993-b790-c9ea97df3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "945675b1-e919-4d67-b5e2-11c1a6f45fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nLangSmith Overview and User Guide | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\nSkip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what\\'s going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith\\'s built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?\\u200bIf a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?\\u200bBuilding and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debugging\\u200bIn the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we\\'ve added a “Share” button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examples\\u200bMost of the time we go to debug, it\\'s because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, it’s often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you\\'re wandering blind. You don’t have any examples to benchmark your changes against.LangSmith addresses this problem by including an “Add to Dataset” button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation\\u200bInitially, we do most of our evaluation manually and ad hoc. We pass in different\\ninputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.\\nYou can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We\\'re eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780b602-9655-4539-9910-fc8789756a97",
   "metadata": {},
   "source": [
    "### index loaded data into a vectore store:\n",
    "- This requires 2 main components: **embedding model** and a **vectorstore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ff44de7-df2b-41a2-b746-5d6fb9334a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embedding-model instance\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "998acbe0-d0f3-41d0-b462-d63f790d6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vector index and vectorstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "836f2230-8521-4743-bacb-42e24e9cf5f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith's built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?\\u200bIf a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?\\u200bBuilding and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debugging\\u200bIn the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we've added a “Share” button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examples\\u200bMost of the time we go to debug, it's because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, it’s often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you're wandering blind. You don’t have any examples to benchmark your changes against.LangSmith addresses this problem by including an “Add to Dataset” button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluation\\u200bInitially, we do most of our evaluation manually and ad hoc. We pass in different\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bf870b5-4f87-4297-9e08-c73990ab9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "196e81be-db49-430e-8d3d-8c2f1cb56850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x0000026349D64EE0>\n"
     ]
    }
   ],
   "source": [
    "print( vector )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5903d-cca0-4504-bc9b-49a4f68d393e",
   "metadata": {},
   "source": [
    "- This `vector` variable contains our vector store of the web-page document.\n",
    "- Now that we have this data indexed in a vectorstore, we will create a retrieval chain.\n",
    "- This chain will perform the following things in sequence:\n",
    "    1. Take an incoming question,\n",
    "    2. Look up relevant documents,\n",
    "    3. Pass those relevant documents along with the original question into an LLM and ask it to answer the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab94177b-70ef-4220-89e8-c7883761cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c15f5-2f78-4f6e-b709-584947c4ce0f",
   "metadata": {},
   "source": [
    "- Now, we want the documents to first come from the retriever we just set up. By doing this, for a given question we can use the retriever to dynamically select the most relevant documents and pass those in as context in the `{context}` placeholder of the above `prompt`, so that only relevant smal-sized context is passed to the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25d93c7f-053b-48bc-b460-e4c1636b442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# This \"retriever\" will take user \"input\" and return the related data according to the question, which will be passed as \"context\" to the \"document_chain\"\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fef2eb-88a7-4cfb-b8dd-da6ca593ca59",
   "metadata": {},
   "source": [
    "- We can now invoke this `retrieval_chain` chain, created above.\n",
    "- This chain returns a Python dictionary in the response.\n",
    "- The response dictionary returned from the LLM has a key named `answer`, which contains the answer to the question asked by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "048c3447-fdc8-4569-8348-ec0ffbb061a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"How can langsmith help with testing?\"\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": user_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88ca6ce8-ec02-43a6-8770-daba3d92bc00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How can langsmith help with testing?',\n",
       " 'context': [Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': 'LangSmith can help with testing by providing the ability to run chains over data points and visualize the outputs. It makes it easy to pull down a dataset, run a chain over the data points, and log the results to a new project associated with the dataset. This allows for reviewing and evaluating the outputs. LangSmith also offers a set of evaluators that can be specified during a test run to evaluate the results. Additionally, LangSmith provides annotation queues that allow for manual review and annotation of runs, which is useful for assessing subjective qualities and validating auto-evaluated runs.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb0a3b8a-8f2b-4a2b-84c1-322c853a4abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print( type(response) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65c4446b-e306-4c94-8efc-fb2a7f20ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'context', 'answer'])\n"
     ]
    }
   ],
   "source": [
    "print( response.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9de33ce-a11e-4ce3-adfd-00a56b48e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing the ability to run chains over data points and visualize the outputs. It makes it easy to pull down a dataset, run a chain over the data points, and log the results to a new project associated with the dataset. This allows for reviewing and evaluating the outputs. LangSmith also offers a set of evaluators that can be specified during a test run to evaluate the results. Additionally, LangSmith provides annotation queues that allow for manual review and annotation of runs, which is useful for assessing subjective qualities and validating auto-evaluated runs.\n"
     ]
    }
   ],
   "source": [
    "# The answer as returned by LLM\n",
    "print( response[\"answer\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50745279-093d-4b74-b9fe-622caf59a8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can langsmith help with testing?\n"
     ]
    }
   ],
   "source": [
    "print( response[\"input\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9022770-f599-46d4-a2e1-332514ca198b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]\n"
     ]
    }
   ],
   "source": [
    "# We can view what relevant pieces are being selected and are being passed to the llm as context\n",
    "\n",
    "print( response[\"context\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e498a-c4ca-431e-8f0e-0c5a1b5e5f65",
   "metadata": {},
   "source": [
    "# Conversation Retrieval Chain:\n",
    "---------------------------------\n",
    "- The chain we've created so far can only answer single questions. But most common LLM applications are chatbots, which are context-aware, as well as, they take whole conversation history into account in their context, such that they can answer follow-up questions.\n",
    "- To include conversation history also into the context, we can still use the `create_retrieval_chain()` function, but we need to change two things:\n",
    "    1. <u>**Updating Retrieval**</u>: The **retrieval method** should now not just work on the most recent input, but rather should take the whole history into account.\n",
    "    2. <u>**Updating Chain**</u>: The **final LLM chain** should likewise take the whole history into account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103e122-88bd-493d-87c0-e503ec13063f",
   "metadata": {},
   "source": [
    "### Conversation Retrieval made with `create_retrieval_chain()` function [it is a retriever, and not a chain] must have following placeholder's in the prompt used in them:\n",
    "- `{chat_history}` : This includes the related data as context, as well as the previous user messages.\n",
    "- `{input}` : Current user question / query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd344c2-329b-42cc-85a4-8123c2d2512f",
   "metadata": {},
   "source": [
    "### 1. Updating Retrieval\n",
    "\n",
    "- This **retrieval <u>cannout be made</u> using only the FAISS vector as `vector.as_retriever()`**\n",
    "- A new retrieval should be made that will take into account conversation details / chat history.\n",
    "- In order to update retrieval, we will create a new chain. This chain will take in the most recent input (`input`) and the conversation history (`chat_history`) and use an LLM to generate a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "671351a2-7137-461b-930a-f5255cf27685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "# Obtained retriever in above case will be used here\n",
    "# retriever = vector.as_retriever()\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37db988-bba2-47b6-99d0-697ce045867c",
   "metadata": {},
   "source": [
    "### Testing the new retriever:\n",
    "- We can test this out by passing in an instance where the user is asking a follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6488fe6-9b5f-4ced-8d0a-8f4088bfcc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "response = history_aware_retriever.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878b47f-6092-4c80-a604-f5e89c931ae6",
   "metadata": {},
   "source": [
    "### The object returned by retriever chain made using `create_history_aware_retriever()` is a `List`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e301e820-033a-4cd8-8f5f-7d666ece8134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print( type(response) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98c9d9dc-a32e-4e4a-9c14-b3e15357d267",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith' metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}\n",
      "\n",
      "\n",
      "page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\" metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}\n",
      "\n",
      "\n",
      "page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\" metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}\n",
      "\n",
      "\n",
      "page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.' metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in response:\n",
    "    print(i, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94c83a-0469-448b-bfb5-6813fb463f53",
   "metadata": {},
   "source": [
    "### 2. Updating Chain\n",
    "Now we have the new retriever, i.e., the **history-aware retriever**. Using this, we can create a new chain to continue the conversation with these retrieved documents in account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "977de483-c27a-4a2d-82cc-af504c68d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, conversation_prompt)\n",
    "\n",
    "history_aware_retrieval_chain = create_retrieval_chain(history_aware_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83a9be-6387-48cf-bdee-0da3004ccc9e",
   "metadata": {},
   "source": [
    "- We can now test this out end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17fa0961-43fb-4685-8861-1de889f220e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "chat_response = history_aware_retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b25bed75-865d-4f1f-b21c-1b92bb51e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print( type( chat_response ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "864d0dd7-994b-41d0-a057-a6d0618cc59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['chat_history', 'input', 'context', 'answer'])\n"
     ]
    }
   ],
   "source": [
    "print( chat_response.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "663de5a0-c907-45e5-bf89-e9d3164901a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'), AIMessage(content='Yes!')], 'input': 'Tell me how', 'context': [Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content=\"Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default\\u200bAt LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging\\u200bDebugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?\\u200bLLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?\\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets\\u200bLangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}), Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation\\u200bAutomatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})], 'answer': 'LangSmith can help test your LLM applications in a few ways:\\n\\n1. Dataset Testing: You can use a dataset that you have constructed or curated to test changes to your prompt or chain. LangSmith simplifies dataset uploading and allows you to run the chain over the data points and visualize the outputs. You can review the results, assign feedback to runs, and mark them as correct or incorrect directly in the web app.\\n\\n2. Evaluators: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While these evaluators are not perfect, they can guide your attention to examples that require closer examination.\\n\\n3. Human Evaluation: LangSmith makes it easy to manually review and annotate runs through annotation queues. You can select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. As a reviewer, you can quickly step through the runs, view input and output, and add your feedback. This is particularly useful for assessing subjective qualities like creativity or humor and validating the accuracy of automatic metrics.\\n\\nBy utilizing these testing capabilities, LangSmith helps you ensure the quality and reliability of your LLM applications.'}\n"
     ]
    }
   ],
   "source": [
    "print( chat_response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee117eb2-9e9d-41aa-a13f-3eb652d43391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "answer": "LangSmith can help test your LLM applications in a few ways:\n\n1. Dataset Testing: You can use a dataset that you have constructed or curated to test changes to your prompt or chain. LangSmith simplifies dataset uploading and allows you to run the chain over the data points and visualize the outputs. You can review the results, assign feedback to runs, and mark them as correct or incorrect directly in the web app.\n\n2. Evaluators: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While these evaluators are not perfect, they can guide your attention to examples that require closer examination.\n\n3. Human Evaluation: LangSmith makes it easy to manually review and annotate runs through annotation queues. You can select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. As a reviewer, you can quickly step through the runs, view input and output, and add your feedback. This is particularly useful for assessing subjective qualities like creativity or humor and validating the accuracy of automatic metrics.\n\nBy utilizing these testing capabilities, LangSmith helps you ensure the quality and reliability of your LLM applications.",
       "chat_history": [
        [
         [
          "content",
          "Can LangSmith help test my LLM applications?"
         ],
         [
          "additional_kwargs",
          {}
         ],
         [
          "type",
          "human"
         ],
         [
          "example",
          false
         ]
        ],
        [
         [
          "content",
          "Yes!"
         ],
         [
          "additional_kwargs",
          {}
         ],
         [
          "type",
          "ai"
         ],
         [
          "example",
          false
         ]
        ]
       ],
       "context": [
        [
         [
          "page_content",
          "LangSmith Overview and User Guide | 🦜️🛠️ LangSmith"
         ],
         [
          "metadata",
          {
           "description": "Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.",
           "language": "en",
           "source": "https://docs.smith.langchain.com/overview",
           "title": "LangSmith Overview and User Guide | 🦜️🛠️ LangSmith"
          }
         ],
         [
          "type",
          "Document"
         ]
        ],
        [
         [
          "page_content",
          "Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookRelease NotesOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by default​At LangChain, all of us have LangSmith’s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we don’t even look at the traces, but the 10% of the time that we do… it’s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebugging​Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?​LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string → string (or chat messages → chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?​So you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?​In complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being"
         ],
         [
          "metadata",
          {
           "description": "Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.",
           "language": "en",
           "source": "https://docs.smith.langchain.com/overview",
           "title": "LangSmith Overview and User Guide | 🦜️🛠️ LangSmith"
          }
         ],
         [
          "type",
          "Document"
         ]
        ],
        [
         [
          "page_content",
          "You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring​After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.We’ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing — mirroring the debug mode approach.We’ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasets​LangSmith makes it easy to curate datasets. However, these aren’t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.↩PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright © 2024 LangChain, Inc."
         ],
         [
          "metadata",
          {
           "description": "Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.",
           "language": "en",
           "source": "https://docs.smith.langchain.com/overview",
           "title": "LangSmith Overview and User Guide | 🦜️🛠️ LangSmith"
          }
         ],
         [
          "type",
          "Document"
         ]
        ],
        [
         [
          "page_content",
          "inputs, and see what happens. At some point though, our application is performing\nwell and we want to be more rigorous about testing changes. We can use a dataset\nthat we’ve constructed along the way (see above). Alternatively, we could spend some\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We've made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we've added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If we’re being honest, most of these evaluators aren't perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluation​Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later."
         ],
         [
          "metadata",
          {
           "description": "Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.",
           "language": "en",
           "source": "https://docs.smith.langchain.com/overview",
           "title": "LangSmith Overview and User Guide | 🦜️🛠️ LangSmith"
          }
         ],
         [
          "type",
          "Document"
         ]
        ]
       ],
       "input": "Tell me how"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "JSON( chat_response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19a5f30c-f6d6-49a5-a488-97cf65006eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       [
        [
         "content",
         "Can LangSmith help test my LLM applications?"
        ],
        [
         "additional_kwargs",
         {}
        ],
        [
         "type",
         "human"
        ],
        [
         "example",
         false
        ]
       ],
       [
        [
         "content",
         "Yes!"
        ],
        [
         "additional_kwargs",
         {}
        ],
        [
         "type",
         "ai"
        ],
        [
         "example",
         false
        ]
       ]
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON( chat_response[\"chat_history\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b7827-18b1-4405-9ace-32de5d5a4c8f",
   "metadata": {},
   "source": [
    "## Continuos Chat Program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5aff0ac-f9d9-43c8-b911-2c37b4edb2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b12a8d32-dda6-4042-bf04-5350374a9f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:   what is langsmith?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  LangSmith is a tool developed by LangChain to simplify the process of building reliable Language Model (LLM) applications. It serves as a tactical user guide and provides effective ways to use LangSmith and maximize its benefits. LangSmith offers features such as tracing, debugging, monitoring, collecting examples, testing and evaluation, collaborative debugging, and exporting datasets. It helps developers understand and optimize the behavior of LLMs, chains, agents, and retrievers, and provides insights into input/output data, prompt editing, sequence of events, latency analysis, token usage, and more.\n",
      "========================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:   Give 5 of its topmost features in bullet points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Certainly! Here are five top features of LangSmith:\n",
      "\n",
      "1. Tracing: LangSmith provides built-in tracing capabilities that visualize the sequence of events in complicated chains and agents, making it easier to understand the interactions and inputs/outputs of each call.\n",
      "\n",
      "2. Debugging: With LangSmith, you can debug LLMs, chains, and agents by accessing the exact input to the LLM, understanding the impact of prompt editing on the output, and identifying the cause of slow chain execution.\n",
      "\n",
      "3. Monitoring: LangSmith allows you to monitor your application by logging all traces, visualizing latency and token usage statistics, and troubleshooting specific issues as they arise. It also supports adding feedback programmatically and tracking performance over time.\n",
      "\n",
      "4. Collecting Examples: LangSmith enables you to collect and add examples to datasets for future testing and evaluation. You can edit the examples before adding them, making it valuable for tracking failures, testing different components, and benchmarking changes.\n",
      "\n",
      "5. Exporting Datasets: LangSmith makes it easy to curate datasets and export them for use in other contexts, such as OpenAI Evals or fine-tuning with tools like FireworksAI. This feature enhances the versatility and usability of datasets beyond LangSmith itself.\n",
      "\n",
      "These features collectively enhance the development, evaluation, and optimization of LLM applications, providing developers with valuable insights and tools for building reliable and high-quality models.\n",
      "========================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:   Explain the 3rd bullet point above\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Certainly! The third bullet point refers to the monitoring capabilities of LangSmith. Once your application is ready to go into production, LangSmith can be used to monitor its performance. Here's a breakdown of the features related to monitoring in LangSmith:\n",
      "\n",
      "- Logging Traces: LangSmith allows you to log all traces of your application, capturing important information about the calls made to LLMs, chains, agents, tools, and retrievers. This comprehensive logging helps in understanding the behavior of your application and diagnosing any issues that may arise.\n",
      "\n",
      "- Visualizing Latency and Token Usage Statistics: LangSmith provides visualizations of latency and token usage statistics. You can analyze the time taken by each step in a chain or agent, identify potential bottlenecks, and optimize the performance of your application. Additionally, you can track the token usage of each step, helping you identify costly parts of the chain and optimize token efficiency.\n",
      "\n",
      "- Troubleshooting and Feedback: LangSmith allows you to troubleshoot specific issues as they arise in your application. You can assign string tags or key-value metadata to each run, enabling you to attach correlation IDs or AB test variants. This makes it easier to filter and analyze runs based on specific criteria. Additionally, you can programmatically associate feedback with runs, such as user feedback obtained through a thumbs up/down button. This feedback can be used to track performance over time, pinpoint underperforming data points, and make improvements to your models or datasets.\n",
      "\n",
      "Overall, LangSmith's monitoring capabilities provide you with insights and tools to track and optimize the performance of your application, ensuring it meets your quality and efficiency requirements in a production environment.\n",
      "========================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:   quit\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    user_question = input(\"User:  \")\n",
    "    if user_question in (\"quit\", \"exit\", \"close\"):\n",
    "        break\n",
    "\n",
    "    chat_response = history_aware_retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": user_question\n",
    "    })\n",
    "\n",
    "    print(\"Assistant: \", chat_response[\"answer\"])\n",
    "    print(\"========================================================================================\", end=\"\\n\\n\\n\")\n",
    "\n",
    "    # Appending the current question and answer to chat_history\n",
    "    chat_history.append( HumanMessage( content=user_question ) )\n",
    "    chat_history.append( AIMessage( content=chat_response[\"answer\"] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0dd45a4-8ff0-4715-b92e-161c2467a910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'langchain_core.messages.human.HumanMessage'>\n",
      "content='what is langsmith?'\n",
      "\n",
      "\n",
      "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangSmith is a tool developed by LangChain to simplify the process of building reliable Language Model (LLM) applications. It serves as a tactical user guide and provides effective ways to use LangSmith and maximize its benefits. LangSmith offers features such as tracing, debugging, monitoring, collecting examples, testing and evaluation, collaborative debugging, and exporting datasets. It helps developers understand and optimize the behavior of LLMs, chains, agents, and retrievers, and provides insights into input/output data, prompt editing, sequence of events, latency analysis, token usage, and more.'\n",
      "\n",
      "\n",
      "Type: <class 'langchain_core.messages.human.HumanMessage'>\n",
      "content='Give 5 of its topmost features in bullet points'\n",
      "\n",
      "\n",
      "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Certainly! Here are five top features of LangSmith:\\n\\n1. Tracing: LangSmith provides built-in tracing capabilities that visualize the sequence of events in complicated chains and agents, making it easier to understand the interactions and inputs/outputs of each call.\\n\\n2. Debugging: With LangSmith, you can debug LLMs, chains, and agents by accessing the exact input to the LLM, understanding the impact of prompt editing on the output, and identifying the cause of slow chain execution.\\n\\n3. Monitoring: LangSmith allows you to monitor your application by logging all traces, visualizing latency and token usage statistics, and troubleshooting specific issues as they arise. It also supports adding feedback programmatically and tracking performance over time.\\n\\n4. Collecting Examples: LangSmith enables you to collect and add examples to datasets for future testing and evaluation. You can edit the examples before adding them, making it valuable for tracking failures, testing different components, and benchmarking changes.\\n\\n5. Exporting Datasets: LangSmith makes it easy to curate datasets and export them for use in other contexts, such as OpenAI Evals or fine-tuning with tools like FireworksAI. This feature enhances the versatility and usability of datasets beyond LangSmith itself.\\n\\nThese features collectively enhance the development, evaluation, and optimization of LLM applications, providing developers with valuable insights and tools for building reliable and high-quality models.'\n",
      "\n",
      "\n",
      "Type: <class 'langchain_core.messages.human.HumanMessage'>\n",
      "content='Explain the 3rd bullet point above'\n",
      "\n",
      "\n",
      "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"Certainly! The third bullet point refers to the monitoring capabilities of LangSmith. Once your application is ready to go into production, LangSmith can be used to monitor its performance. Here's a breakdown of the features related to monitoring in LangSmith:\\n\\n- Logging Traces: LangSmith allows you to log all traces of your application, capturing important information about the calls made to LLMs, chains, agents, tools, and retrievers. This comprehensive logging helps in understanding the behavior of your application and diagnosing any issues that may arise.\\n\\n- Visualizing Latency and Token Usage Statistics: LangSmith provides visualizations of latency and token usage statistics. You can analyze the time taken by each step in a chain or agent, identify potential bottlenecks, and optimize the performance of your application. Additionally, you can track the token usage of each step, helping you identify costly parts of the chain and optimize token efficiency.\\n\\n- Troubleshooting and Feedback: LangSmith allows you to troubleshoot specific issues as they arise in your application. You can assign string tags or key-value metadata to each run, enabling you to attach correlation IDs or AB test variants. This makes it easier to filter and analyze runs based on specific criteria. Additionally, you can programmatically associate feedback with runs, such as user feedback obtained through a thumbs up/down button. This feedback can be used to track performance over time, pinpoint underperforming data points, and make improvements to your models or datasets.\\n\\nOverall, LangSmith's monitoring capabilities provide you with insights and tools to track and optimize the performance of your application, ensuring it meets your quality and efficiency requirements in a production environment.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for obj in chat_history:\n",
    "    print( f\"Type: {type(obj)}\", obj, sep=\"\\n\", end=\"\\n\\n\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bb1e1-b8fa-47bb-91f4-ca8a44800806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
